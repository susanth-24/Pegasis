{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:50.138041Z","iopub.status.busy":"2023-07-16T04:06:50.137709Z","iopub.status.idle":"2023-07-16T04:06:54.816335Z","shell.execute_reply":"2023-07-16T04:06:54.815262Z","shell.execute_reply.started":"2023-07-16T04:06:50.138013Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","\n","class Backbone(nn.Module):\n","    def __init__(self):\n","        super(Backbone, self).__init__()\n","        self.base_model = models.mobilenet_v2(pretrained=True)\n","        print('Base model loaded: MobileNetV2')\n","\n","        self.block1 = nn.Sequential(*list(self.base_model.features[:5]))\n","        self.block2 = nn.Sequential(*list(self.base_model.features[5:8]))\n","        self.block4 = nn.Sequential(*list(self.base_model.features[8:15]))\n","        self.block7 = nn.Sequential(*list(self.base_model.features[15:17]))\n","        self.block14 = nn.Sequential(*list(self.base_model.features[17:-1]))\n","\n","    def forward(self, x):\n","        features_block1 = self.block1(x)\n","        features_block2 = self.block2(features_block1)\n","        features_block4 = self.block4(features_block2)\n","        features_block7 = self.block7(features_block4)\n","        features_block14 = self.block14(features_block7)\n","\n","        return features_block1, features_block2, features_block4, features_block7, features_block14\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:54.821425Z","iopub.status.busy":"2023-07-16T04:06:54.820554Z","iopub.status.idle":"2023-07-16T04:06:54.864798Z","shell.execute_reply":"2023-07-16T04:06:54.863448Z","shell.execute_reply.started":"2023-07-16T04:06:54.821389Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import math\n","\n","class BiFPN(nn.Module):\n","    def __init__(self,  fpn_sizes):\n","        super(BiFPN, self).__init__()\n","\n","        P3_channels, P4_channels, P5_channels, P6_channels, P7_channels = fpn_sizes\n","        self.W_bifpn = 64\n","\n","        #self.p6_td_conv  = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_td_conv  = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p6_td_conv_2  = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_td_act   = nn.ReLU()\n","        self.p6_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p6_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_td_w2    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","\n","        self.p5_td_conv  = nn.Conv2d(P5_channels,self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p5_td_conv_2  = nn.Conv2d(self.W_bifpn,self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p5_td_act   = nn.ReLU()\n","        self.p5_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p5_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_td_w2    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","\n","        self.p4_td_conv  = nn.Conv2d(P4_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p4_td_conv_2  = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p4_td_act   = nn.ReLU()\n","        self.p4_td_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p4_td_w1    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_td_w2    = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_upsample   = nn.Upsample(scale_factor=2, mode='nearest')\n","\n","\n","        self.p3_out_conv = nn.Conv2d(P3_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p3_out_conv_2 = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p3_out_act   = nn.ReLU()\n","        self.p3_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p3_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p3_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_upsample  = nn.Upsample(scale_factor=2, mode='nearest')\n","\n","        #self.p4_out_conv = nn.Conv2d(P4_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p4_out_conv = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p4_out_act   = nn.ReLU()\n","        self.p4_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p4_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_out_w3   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p3_downsample= nn.MaxPool2d(kernel_size=2)\n","\n","        #self.p5_out_conv = nn.Conv2d(P5_channels,self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p5_out_conv = nn.Conv2d(self.W_bifpn,self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p5_out_act   = nn.ReLU()\n","        self.p5_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p5_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p5_out_w3   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p4_downsample= nn.MaxPool2d(kernel_size=2)\n","\n","        #self.p6_out_conv = nn.Conv2d(P6_channels, self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p6_out_conv = nn.Conv2d(self.W_bifpn, self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p6_out_act   = nn.ReLU()\n","        self.p6_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p6_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p6_out_w3   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        #self.p4_downsample= nn.MaxPool2d(kernel_size=2)\n","\n","\n","        self.p7_out_conv = nn.Conv2d(P7_channels,self.W_bifpn, kernel_size=3, stride=1, bias=True, padding=1)\n","        self.p7_out_conv_2 = nn.Conv2d(self.W_bifpn,self.W_bifpn, kernel_size=3, stride=1, groups=self.W_bifpn, bias=True, padding=1)\n","        self.p7_out_act  = nn.ReLU()\n","        self.p7_out_conv_bn = nn.BatchNorm2d(self.W_bifpn)\n","        self.p7_out_w1   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","        self.p7_out_w2   = torch.tensor(1, dtype=torch.float, requires_grad=True)\n","\n","\n","    def forward(self, inputs):\n","        epsilon = 0.0001\n","        P3, P4, P5, P6, P7 = inputs\n","        #print (\"Input::\", P3.shape, P4.shape, P5.shape, P6.shape, P7.shape)\n","        #P6_td = self.p6_td_conv((self.p6_td_w1 * P6 ) /\n","        #                         (self.p6_td_w1 + epsilon))\n","\n","        P7_td  = self.p7_out_conv(P7)\n","\n","        P6_td_inp = self.p6_td_conv(P6)\n","        P6_td = self.p6_td_conv_2((self.p6_td_w1 * P6_td_inp + self.p6_td_w2 * P7_td) /\n","                                 (self.p6_td_w1 + self.p6_td_w2 + epsilon))\n","        #P6_td = self.p6_td_conv_2(P6_td_inp)\n","        P6_td = self.p6_td_act(P6_td)\n","        P6_td = self.p6_td_conv_bn(P6_td)\n","\n","\n","        P5_td_inp = self.p5_td_conv(P5)\n","        #print (P5_td_inp.shape, P6_td.shape)\n","        P5_td = self.p5_td_conv_2((self.p5_td_w1 * P5_td_inp + self.p5_td_w2 * P6_td) /\n","                                 (self.p5_td_w1 + self.p5_td_w2 + epsilon))\n","        P5_td = self.p5_td_act(P5_td)\n","        P5_td = self.p5_td_conv_bn(P5_td)\n","\n","        #print (P4.shape, P5_td.shape)\n","        P4_td_inp = self.p4_td_conv(P4)\n","        P4_td = self.p4_td_conv_2((self.p4_td_w1 * P4_td_inp + self.p4_td_w2 * self.p5_upsample(P5_td)) /\n","                                 (self.p4_td_w1 + self.p4_td_w2 + epsilon))\n","        P4_td = self.p4_td_act(P4_td)\n","        P4_td = self.p4_td_conv_bn(P4_td)\n","\n","\n","        P3_td  = self.p3_out_conv(P3)\n","        P3_out = self.p3_out_conv_2((self.p3_out_w1 * P3_td + self.p3_out_w2 * self.p4_upsample(P4_td)) /\n","                                 (self.p3_out_w1 + self.p3_out_w2 + epsilon))\n","        P3_out = self.p3_out_act(P3_out)\n","        P3_out = self.p3_out_conv_bn(P3_out)\n","\n","        #print (P4_td.shape, P3_out.shape)\n","\n","        P4_out = self.p4_out_conv((self.p4_out_w1 * P4_td_inp  + self.p4_out_w2 * P4_td + self.p4_out_w3 * self.p3_downsample(P3_out) )\n","                                    / (self.p4_out_w1 + self.p4_out_w2 + self.p4_out_w3 + epsilon))\n","        P4_out = self.p4_out_act(P4_out)\n","        P4_out = self.p4_out_conv_bn(P4_out)\n","\n","\n","        P5_out = self.p5_out_conv(( self.p5_out_w1 * P5_td_inp + self.p5_out_w2 * P5_td + self.p5_out_w3 * self.p4_downsample(P4_out) )\n","                                    / (self.p5_out_w2 + self.p5_out_w3 + epsilon))\n","        P5_out = self.p5_out_act(P5_out)\n","        P5_out = self.p5_out_conv_bn(P5_out)\n","\n","\n","        P6_out = self.p6_out_conv((self.p6_out_w1 * P6_td_inp + self.p6_out_w2 * P6_td + self.p6_out_w3 * (P5_out) )\n","                                    / (self.p6_out_w1 + self.p6_out_w2 + self.p6_out_w3 + epsilon))\n","        P6_out = self.p6_out_act(P6_out)\n","        P6_out = self.p6_out_conv_bn(P6_out)\n","\n","\n","        P7_out = self.p7_out_conv_2((self.p7_out_w1 * P7_td + self.p7_out_w2 * P6_out) /\n","                                 (self.p7_out_w1 + self.p7_out_w2 + epsilon))\n","        P7_out = self.p7_out_act(P7_out)\n","        P7_out = self.p7_out_conv_bn(P7_out)\n","\n","\n","        return [P3_out, P4_out, P5_out, P6_out, P7_out]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:54.866970Z","iopub.status.busy":"2023-07-16T04:06:54.866366Z","iopub.status.idle":"2023-07-16T04:06:54.886737Z","shell.execute_reply":"2023-07-16T04:06:54.885713Z","shell.execute_reply.started":"2023-07-16T04:06:54.866937Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import models\n","import torch.nn.functional as F\n","\n","class UpSample(nn.Sequential):\n","    def __init__(self, skip_input, output_features):\n","        super(UpSample, self).__init__()        \n","        self.convA = nn.Conv2d(skip_input, output_features, kernel_size=3, stride=1, padding=1)\n","        self.leakyreluA = nn.LeakyReLU(0.2)\n","        self.convB = nn.Conv2d(output_features, output_features, kernel_size=3, stride=1, padding=1)\n","        self.leakyreluB = nn.LeakyReLU(0.2)\n","\n","    def forward(self, x, concat_with):\n","        up_x = F.interpolate(x, size=[concat_with.size(2), concat_with.size(3)], mode='bilinear', align_corners=True)\n","        return self.leakyreluB( self.convB( self.convA( torch.cat([up_x, concat_with], dim=1)  ) )  )\n","\n","class Decoder(nn.Module):\n","    def __init__(self, num_features=64, decoder_width = 1.0):\n","        super(Decoder, self).__init__()\n","        features = int(num_features * decoder_width)\n","\n","        self.conv2 = nn.Conv2d(num_features, features, kernel_size=1, stride=1, padding=0)\n","\n","        self.up1 = UpSample(skip_input=features//1 + 64, output_features=features//2)\n","        self.up2 = UpSample(skip_input=features//2 + 64,  output_features=features//4)\n","        self.up3 = UpSample(skip_input=features//4 + 64,  output_features=features//8)\n","        self.up4 = UpSample(skip_input=features//8 + 64,  output_features=features//16)\n","\n","        self.conv3 = nn.Conv2d(features//16, 1, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, features):\n","        x_block0, x_block1, x_block2, x_block3, x_block4 = features[0], features[1], features[2], features[3], features[4]\n","        x_d0 = self.conv2(F.relu(x_block4))\n","\n","        x_d1 = self.up1(x_d0, x_block3)\n","        x_d2 = self.up2(x_d1, x_block2)\n","        x_d3 = self.up3(x_d2, x_block1)\n","        x_d4 = self.up4(x_d3, x_block0)\n","        output = F.interpolate(x_d4, size=(480, 640), mode='bilinear', align_corners=True)\n","\n","        return self.conv3(output)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:54.889554Z","iopub.status.busy":"2023-07-16T04:06:54.888964Z","iopub.status.idle":"2023-07-16T04:06:54.901495Z","shell.execute_reply":"2023-07-16T04:06:54.900292Z","shell.execute_reply.started":"2023-07-16T04:06:54.889519Z"},"trusted":true},"outputs":[],"source":["class Depth_Model(nn.Module):\n","    def __init__(self):\n","        super(Depth_Model, self).__init__()\n","        self.back = Backbone()\n","        self.fusion = BiFPN([32, 64, 160, 160, 320])\n","        self.decoder = Decoder()\n","\n","    def forward(self, x):\n","        features = self.back(x)\n","        fused_features = self.fusion(features)\n","        output = self.decoder(fused_features)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-13T15:14:07.949749Z","iopub.status.idle":"2023-07-13T15:14:07.957118Z","shell.execute_reply":"2023-07-13T15:14:07.956904Z","shell.execute_reply.started":"2023-07-13T15:14:07.956881Z"},"trusted":true},"outputs":[],"source":["!pip install kornia"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-13T15:14:07.958432Z","iopub.status.idle":"2023-07-13T15:14:07.959146Z","shell.execute_reply":"2023-07-13T15:14:07.958939Z","shell.execute_reply.started":"2023-07-13T15:14:07.958917Z"},"trusted":true},"outputs":[],"source":["import torch\n","import kornia.metrics as metrics\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import numpy as np\n","from torch.autograd import Variable\n","\n","def ssim(\n","    img1: torch.Tensor,\n","    img2: torch.Tensor,\n","    window_size: int,\n","    max_val: float = 100.0,\n","    eps: float = 1e-12,\n","    reduction: str = 'mean',\n","    padding: str = 'same',\n",") -> torch.Tensor:\n","    # Compute the SSIM map\n","    ssim_map = metrics.ssim(img1, img2, window_size, max_val, eps, padding)\n","\n","    # Compute and reduce the loss\n","    loss = torch.clamp((1.0 - ssim_map) / 2, min=0, max=1)\n","\n","    if reduction == 'mean':\n","        loss = torch.mean(loss)\n","    elif reduction == 'sum':\n","        loss = torch.sum(loss)\n","    elif reduction == 'none':\n","        pass\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T09:56:51.093406Z","iopub.status.busy":"2023-07-13T09:56:51.093051Z","iopub.status.idle":"2023-07-13T09:56:51.471495Z","shell.execute_reply":"2023-07-13T09:56:51.470483Z","shell.execute_reply.started":"2023-07-13T09:56:51.093351Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from PIL import Image\n","import pandas as pd\n","from sklearn.utils import shuffle\n","import os\n","\n","class NYUV2Dataset(torch.utils.data.Dataset):\n","    def __init__(self, csv_file, base_dir, output_shape, transform=None):\n","        self.data = pd.read_csv(csv_file)\n","        self.base_dir = base_dir\n","        self.output_shape = output_shape\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        rgb_path = os.path.join(self.base_dir, self.data.iloc[idx, 0])\n","        depth_path = os.path.join(self.base_dir, self.data.iloc[idx, 1])\n","        \n","        rgb_image = Image.open(rgb_path).convert('RGB')\n","        depth_image = Image.open(depth_path).convert('L')\n","        \n","        # Resize the depth image\n","        depth_image = depth_image.resize((self.output_shape[3], self.output_shape[2]))\n","        \n","        if self.transform is not None:\n","            rgb_image = self.transform(rgb_image)\n","        \n","        depth_transform = transforms.Compose([\n","            transforms.ToTensor()\n","        ])\n","        depth_image = depth_transform(depth_image)\n","\n","        return rgb_image, depth_image\n","\n","\n","\n","# Set the paths to the NYU v2 dataset CSV file and the corresponding RGB and depth images\n","csv_file = '/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv'\n","base_dir = '/kaggle/input/nyu-depth-v2/nyu_data/'\n","\n","# Define the transformation to apply to the images\n","transform = transforms.Compose([\n","    transforms.Resize((480, 640)),\n","    transforms.ToTensor()\n","])\n","batch_size = 16\n","output_shape = (batch_size, 1, 480, 640)\n","nyu_dataset = NYUV2Dataset(csv_file, base_dir=base_dir, output_shape=output_shape, transform=transform)\n","\n","\n","# Create a data loader to load the dataset in batches\n","\n","data_loader = torch.utils.data.DataLoader(nyu_dataset, batch_size=batch_size, shuffle=True)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["model.load_state_dict(torch.load('/content/drive/MyDrive/kaggle/depth_model_epoch_6_v1_2.pth'))\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:54.903296Z","iopub.status.busy":"2023-07-16T04:06:54.902959Z","iopub.status.idle":"2023-07-16T04:06:58.551863Z","shell.execute_reply":"2023-07-16T04:06:58.550896Z","shell.execute_reply.started":"2023-07-16T04:06:54.903265Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n","100%|██████████| 13.6M/13.6M [00:00<00:00, 84.6MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Base model loaded: MobileNetV2\n"]}],"source":["model = Depth_Model().cuda()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T15:15:03.560475Z","iopub.status.busy":"2023-07-13T15:15:03.560101Z","iopub.status.idle":"2023-07-13T15:15:03.567270Z","shell.execute_reply":"2023-07-13T15:15:03.566192Z","shell.execute_reply.started":"2023-07-13T15:15:03.560444Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.0001\n","optimizer = torch.optim.Adam( model.parameters(), learning_rate )"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:06:58.555728Z","iopub.status.busy":"2023-07-16T04:06:58.555413Z","iopub.status.idle":"2023-07-16T04:06:58.563009Z","shell.execute_reply":"2023-07-16T04:06:58.562145Z","shell.execute_reply.started":"2023-07-16T04:06:58.555703Z"},"trusted":true},"outputs":[],"source":["def DepthNorm(depth, maxDepth=1000.0): \n","    return maxDepth / depth\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:07:11.504266Z","iopub.status.busy":"2023-07-16T04:07:11.503838Z","iopub.status.idle":"2023-07-16T04:07:11.831212Z","shell.execute_reply":"2023-07-16T04:07:11.830364Z","shell.execute_reply.started":"2023-07-16T04:07:11.504229Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["model.load_state_dict(torch.load('/kaggle/input/pegasisv3-35/pegasisv3_epoch_35.pth'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T09:57:23.843931Z","iopub.status.busy":"2023-07-13T09:57:23.843055Z","iopub.status.idle":"2023-07-13T10:29:47.377226Z","shell.execute_reply":"2023-07-13T10:29:47.376300Z","shell.execute_reply.started":"2023-07-13T09:57:23.843877Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","epochs = 5\n","l1_criterion = nn.L1Loss()\n","\n","for epoch in range(epochs):\n","    total_loss = 0.0\n","    pbar = tqdm(data_loader, total=len(data_loader), desc=f\"Epoch {epoch+1}/{epochs}\")\n","    model.train()\n","    for batch in pbar:\n","        optimizer.zero_grad()\n","        inputs, targets = batch\n","        image = inputs.cuda()\n","        depth = targets.cuda(non_blocking=True)\n","        \n","        depth_n = DepthNorm( depth )\n","\n","        output = model(image)\n","        output.shape\n","        depth_n.shape\n","        l_depth = l1_criterion(output, depth_n)\n","        l_ssim = torch.clamp((1 - ssim(output, depth_n, 5)) * 0.5, 0, 1)\n","        loss = (1.0 * l_ssim.mean().item()) + (0.1 * l_depth)\n","        print(l_ssim.mean().item(),l_depth)\n","        #loss = (1.0 * l_ssim) + (0.1 * l_depth)\n","\n","\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","\n","        pbar.set_postfix({'Loss': loss.item()})\n","\n","    average_loss = total_loss / len(data_loader)\n","    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {average_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T10:31:00.912225Z","iopub.status.busy":"2023-07-13T10:31:00.911783Z","iopub.status.idle":"2023-07-13T10:31:01.029725Z","shell.execute_reply":"2023-07-13T10:31:01.028774Z","shell.execute_reply.started":"2023-07-13T10:31:00.912187Z"},"trusted":true},"outputs":[],"source":["# Save the weights of the model\n","torch.save(model.state_dict(), '/kaggle/working/pegasisv3_epoch_35.pth')\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:08:01.988417Z","iopub.status.busy":"2023-07-16T04:08:01.988052Z","iopub.status.idle":"2023-07-16T04:08:02.064140Z","shell.execute_reply":"2023-07-16T04:08:02.063186Z","shell.execute_reply.started":"2023-07-16T04:08:01.988386Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms\n","from PIL import Image\n","\n","output_height, output_width = 480, 640\n","\n","image = Image.open('/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_test/00014_colors.png')  # Replace 'your_image.jpg' with the path to your image\n","#image=Image.open('/content/1.jpg')\n","transform = transforms.Compose([\n","    transforms.Resize((output_height, output_width)),\n","    transforms.ToTensor()\n","])\n","image_tensor = transform(image)\n","\n","image_tensor = image_tensor.unsqueeze(0)  # Add a batch dimension\n","image_tensor = image_tensor.permute(0, 2, 3, 1)  # Reshape from (1, 3, 480, 640) to (1, 480, 640, 3)\n","image_tensor = image_tensor.permute(0, 3, 1, 2)  # Reshape to the required shape (1, 3, 480, 640)\n","image_tensor = image_tensor.float()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:15:11.583558Z","iopub.status.busy":"2023-07-16T04:15:11.583208Z","iopub.status.idle":"2023-07-16T04:15:11.632061Z","shell.execute_reply":"2023-07-16T04:15:11.631079Z","shell.execute_reply.started":"2023-07-16T04:15:11.583527Z"},"trusted":true},"outputs":[],"source":["import torchvision.transforms as transforms\n","from PIL import Image\n","\n","output_height, output_width = 480, 640\n","\n","image_1 = Image.open('/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_test/00014_depth.png')  # Replace 'your_image.jpg' with the path to your image\n","#image=Image.open('/content/1.jpg')\n","transform = transforms.Compose([\n","    transforms.Resize((output_height, output_width)),\n","    transforms.ToTensor()\n","])\n","image_tensor_1 = transform(image_1)\n","\n","image_tensor_1 = image_tensor_1.unsqueeze(0)  # Add a batch dimension\n","image_tensor_1 = image_tensor_1.permute(0, 2, 3, 1)  # Reshape from (1, 3, 480, 640) to (1, 480, 640, 3)\n","image_tensor_1 = image_tensor_1.permute(0, 3, 1, 2)  # Reshape to the required shape (1, 3, 480, 640)\n","image_tensor_1= image_tensor_1.float()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:08:10.913236Z","iopub.status.busy":"2023-07-16T04:08:10.912877Z","iopub.status.idle":"2023-07-16T04:08:16.218672Z","shell.execute_reply":"2023-07-16T04:08:16.217667Z","shell.execute_reply.started":"2023-07-16T04:08:10.913206Z"},"trusted":true},"outputs":[],"source":["with torch.no_grad():\n","    outputv3 = model(image_tensor.to(\"cuda\"))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:20:01.278917Z","iopub.status.busy":"2023-07-16T04:20:01.278449Z","iopub.status.idle":"2023-07-16T04:20:01.288668Z","shell.execute_reply":"2023-07-16T04:20:01.287732Z","shell.execute_reply.started":"2023-07-16T04:20:01.278872Z"},"trusted":true},"outputs":[],"source":["depth_np = DepthNorm( image_tensor_1 )"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:20:05.603092Z","iopub.status.busy":"2023-07-16T04:20:05.602693Z","iopub.status.idle":"2023-07-16T04:20:05.615511Z","shell.execute_reply":"2023-07-16T04:20:05.614234Z","shell.execute_reply.started":"2023-07-16T04:20:05.603052Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[[[0.3482, 0.3482, 0.3483,  ..., 0.7067, 0.7067, 0.7067],\n","          [0.3482, 0.3482, 0.3483,  ..., 0.7067, 0.7067, 0.7067],\n","          [0.3481, 0.3481, 0.3482,  ..., 0.7067, 0.7072, 0.7072],\n","          ...,\n","          [0.5176, 0.5176, 0.5173,  ..., 0.7153, 0.7158, 0.7158],\n","          [0.5176, 0.5173, 0.5173,  ..., 0.7153, 0.7153, 0.7153],\n","          [0.5173, 0.5173, 0.5173,  ..., 0.7148, 0.7153, 0.7153]]]])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["depth_np"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:21:31.593112Z","iopub.status.busy":"2023-07-16T04:21:31.592747Z","iopub.status.idle":"2023-07-16T04:21:31.599443Z","shell.execute_reply":"2023-07-16T04:21:31.598421Z","shell.execute_reply.started":"2023-07-16T04:21:31.593083Z"},"trusted":true},"outputs":[],"source":["outputv3_np = DepthNorm(outputv3 )"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:27:13.327984Z","iopub.status.busy":"2023-07-16T04:27:13.327631Z","iopub.status.idle":"2023-07-16T04:27:13.332969Z","shell.execute_reply":"2023-07-16T04:27:13.331786Z","shell.execute_reply.started":"2023-07-16T04:27:13.327954Z"},"trusted":true},"outputs":[],"source":["outputv3_1=outputv3/1000"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:27:27.508211Z","iopub.status.busy":"2023-07-16T04:27:27.507855Z","iopub.status.idle":"2023-07-16T04:27:27.518273Z","shell.execute_reply":"2023-07-16T04:27:27.517401Z","shell.execute_reply.started":"2023-07-16T04:27:27.508181Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[[[1.8693, 2.4451, 2.4849,  ..., 3.8082, 3.6833, 2.2711],\n","          [2.6236, 3.2389, 3.2808,  ..., 5.0411, 4.8793, 3.2757],\n","          [2.6450, 3.2429, 3.2777,  ..., 5.1362, 4.9835, 3.3463],\n","          ...,\n","          [4.4004, 5.2660, 5.3076,  ..., 6.5036, 6.3927, 4.2874],\n","          [4.4372, 5.2998, 5.3479,  ..., 6.4646, 6.3348, 4.2425],\n","          [3.0797, 3.4012, 3.4350,  ..., 4.0463, 3.9384, 2.6073]]]],\n","       device='cuda:0')"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["outputv3_1"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:29:29.379204Z","iopub.status.busy":"2023-07-16T04:29:29.378480Z","iopub.status.idle":"2023-07-16T04:29:29.388159Z","shell.execute_reply":"2023-07-16T04:29:29.387150Z","shell.execute_reply.started":"2023-07-16T04:29:29.379166Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["SILogE Loss: 0.05573129653930664\n"]}],"source":["import torch\n","import torch.nn.functional as F\n","\n","def siloge_loss(depth_pred, depth_gt):\n","    # Add a small epsilon to avoid division by zero\n","    epsilon = 1e-6\n","\n","    # Logarithm of depth values\n","    log_depth_pred = torch.log(torch.clamp(depth_pred, min=epsilon))\n","    log_depth_gt = torch.log(torch.clamp(depth_gt, min=epsilon))\n","\n","    # Calculate the SILogE loss\n","    siloge = torch.pow(log_depth_gt - log_depth_pred, 2).mean() - torch.pow(log_depth_gt.mean() - log_depth_pred.mean(), 2)\n","\n","    return siloge\n","\n","# Example usage:\n","# depth_pred and depth_gt are torch tensors representing the predicted depth and ground truth depth images, respectively.\n","loss = siloge_loss(outputv3_1, depth_np.to(\"cuda\"))\n","print(\"SILogE Loss:\", loss.item())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","output_reshaped = outputv3_1.squeeze().detach().cpu().numpy()\n","plt.imshow(output_reshaped, cmap='inferno')\n","#plt.imsave(\"/kaggle/working/pegasisv3_ep35.png\",output_reshaped, cmap='inferno')"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:16:25.444170Z","iopub.status.busy":"2023-07-16T04:16:25.443482Z","iopub.status.idle":"2023-07-16T04:16:25.474094Z","shell.execute_reply":"2023-07-16T04:16:25.473171Z","shell.execute_reply.started":"2023-07-16T04:16:25.444136Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[[[1869.3442, 2445.1453, 2484.8867,  ..., 3808.2241, 3683.3252,\n","           2271.1025],\n","          [2623.6228, 3238.9026, 3280.8318,  ..., 5041.0908, 4879.3481,\n","           3275.6704],\n","          [2645.0029, 3242.9011, 3277.7134,  ..., 5136.1919, 4983.5337,\n","           3346.2866],\n","          ...,\n","          [4400.3711, 5265.9624, 5307.5605,  ..., 6503.6406, 6392.6865,\n","           4287.4385],\n","          [4437.1875, 5299.7593, 5347.9072,  ..., 6464.5625, 6334.8232,\n","           4242.4878],\n","          [3079.6970, 3401.2034, 3435.0337,  ..., 4046.2664, 3938.4124,\n","           2607.2720]]]], device='cuda:0')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["outputv3."]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-07-16T04:23:21.558224Z","iopub.status.busy":"2023-07-16T04:23:21.557844Z","iopub.status.idle":"2023-07-16T04:23:21.570938Z","shell.execute_reply":"2023-07-16T04:23:21.569796Z","shell.execute_reply.started":"2023-07-16T04:23:21.558192Z"},"trusted":true},"outputs":[{"data":{"text/plain":["tensor([[[[0.3482, 0.3482, 0.3483,  ..., 0.7067, 0.7067, 0.7067],\n","          [0.3482, 0.3482, 0.3483,  ..., 0.7067, 0.7067, 0.7067],\n","          [0.3481, 0.3481, 0.3482,  ..., 0.7067, 0.7072, 0.7072],\n","          ...,\n","          [0.5176, 0.5176, 0.5173,  ..., 0.7153, 0.7158, 0.7158],\n","          [0.5176, 0.5173, 0.5173,  ..., 0.7153, 0.7153, 0.7153],\n","          [0.5173, 0.5173, 0.5173,  ..., 0.7148, 0.7153, 0.7153]]]],\n","       device='cuda:0')"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["depth_np.to(\"cuda\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
